{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per creare gli embeddings non ci serve dividere il dataset in test e train ma è utile trainare e quindi selezionare le features della matrice in base alle frequenze incontrate  nel corpus. Questo implica che un corpus più ampio ci aiuta ad una migliore selezione considerando che non dovremmo avere limiti a livello computazionale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading\n",
    "data_1 = pd.read_csv(\"Dati/train.csv\")\n",
    "data_2 = pd.read_csv(\"Dati/test.csv\")\n",
    "data = pd.concat([data_1, data_2], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella fase di preprocessing si cerca di diminuire la varianza di termini che può essere creata da typo o comunque contenuto non informativo. Data la natura sparsa del tipo di rappresentazione del testo scelta (TF-IDF) questo aiuta la selezione delle features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    '''\n",
    "    la funzione prende una stringa e la pulisce secondo i criteri scritti nei commenti\n",
    "        input= text\n",
    "        otput= text\n",
    "    '''\n",
    "    #l'obbiettivo è diminuire il numero di termini\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', ' ', text)#eliminare i numeri perchè non hanno informazioni sul sentiment\n",
    "    text = re.sub(r'<.*?>', ' ', text)#elimina il testo utilizzato per html e.g <br \\>\n",
    "    text = re.sub(r'(--+)', ' ', text)  #elimina - quando sono ripetuti\n",
    "    text = re.sub(r\"[;%#'()_:=+,.*|~^<>@{}/?\\\"\\[\\]\\[\\\\]\",\" \",text) # rimuove la punteggiatura e i simboli tranne il punto esclamativo\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creazione delle colonne pulite\n",
    "data['clean_review'] = data['review'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prima del preprocessing possimao individuare 3555 parole che non sono presenti dopo il preprocessing, mentre dopo il preprocessing abbiamo 212 parole che non sono presenti prima del preprocessing\n",
      "{1: 185, 2: 20, 5: 1, 6: 2, 7: 1, 8: 2, 18: 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#review \n",
    "vectorizer = CountVectorizer()\n",
    "count = vectorizer.fit_transform(data['review'].tolist())\n",
    "vocabolario = dict(zip(vectorizer.get_feature_names_out().tolist(),count.sum(axis=0).A1.tolist()))\n",
    "vocabolario_ordinato = dict(sorted(vocabolario.items(), key=lambda x: x[1], reverse=True))\n",
    "#review pulite\n",
    "vectorizer_2= CountVectorizer()\n",
    "count_clean = vectorizer_2.fit_transform(data['clean_review'].tolist())\n",
    "vocabolario_clean = dict(zip(vectorizer_2.get_feature_names_out().tolist(),count_clean.sum(axis=0).A1.tolist()))\n",
    "vocabolario_ordinato_clean = dict(sorted(vocabolario_clean.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "keys_vocabolario = set(vocabolario_ordinato.keys())\n",
    "keys_vocabolario_clean = set(vocabolario_ordinato_clean.keys())\n",
    "\n",
    "keys_in_common = keys_vocabolario.intersection(keys_vocabolario_clean)\n",
    "\n",
    "# Trova le parole presenti solo prima del prerpocessing\n",
    "keys_only_before = keys_vocabolario - keys_vocabolario_clean\n",
    "\n",
    "# Trova le parole presenti solo dopo il preprocessing\n",
    "keys_only_after = keys_vocabolario_clean - keys_vocabolario\n",
    "\n",
    "print(f\"Prima del preprocessing possimao individuare {len(keys_only_before)} parole che non sono presenti dopo il preprocessing, mentre dopo il preprocessing abbiamo {len(keys_only_after)} parole che non sono presenti prima del preprocessing\")\n",
    "\n",
    "conteggio_valori = Counter([vocabolario_ordinato_clean[i] for i in keys_only_after])\n",
    "conteggio_valori = dict(sorted(conteggio_valori.items()))\n",
    "print(conteggio_valori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "#scelgo di pendere i bi-grams perchè potrebbero esserci delle negazioni che sono informative    \n",
    "                                   ngram_range=(1,2), \n",
    "                                   stop_words='english', \n",
    "                                   analyzer='word',\n",
    "#metto questo filtro perchè ci sono typo e parole inventate tipo usernames che andrebbero solo ad appesantire  \n",
    "                                   min_df=0.001, \n",
    "                                   max_df=0.7,\n",
    "                                   sublinear_tf=True, \n",
    "                                   use_idf=True)\n",
    "#creiamo la matrice dei tfidf in base a tutti i documenti del corpus in modo da avere un numero di colonne che rappresentano\n",
    "#quell che in base al corpus potrebbero essere delle parole significative per la comprensione del label\n",
    "tfidf_vectorizer.fit(data['clean_review'])\n",
    "# eliminiamo le review che non hanno il label considerando che vogliamo fare dell'aprendimento supervisionato\n",
    "data_filtered = data.dropna(subset=['label']).copy()\n",
    "#generiamo la matrice in base alle fetures precedentemente create su tutto il corpus e ai documenti con cui vogliamo fare la classificazione\n",
    "data_filtered = data.groupby('label').apply(lambda x: x.sample(2000, replace=False)).reset_index(drop=True)\n",
    "X = tfidf_vectorizer.transform(data_filtered['clean_review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associamo il label ad un valore numerico\n",
    "le = LabelEncoder()\n",
    "data_filtered['label_encoded'] = le.fit_transform(data_filtered['label'])\n",
    "#classificazione\n",
    "y = data_filtered['label_encoded']\n",
    "#kernel choice \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iperparametri ottimali: {'C': 1, 'degree': 1}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#X_cv, X_val, y_cv, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=77, stratify=y)\n",
    "\n",
    "modello = svm.SVC(kernel='poly')\n",
    "\n",
    "# Definizione della griglia degli iperparametri da esplorare\n",
    "parametri = {\n",
    "    'degree': [1, 2, 3, 4, 5],  # Varia il grado del polinomio\n",
    "    'C': [0.1, 1, 10]      # Varia il parametro di costo C\n",
    "}\n",
    "\n",
    "# Creazione dell'oggetto GridSearchCV\n",
    "grid_search = GridSearchCV(modello, parametri, cv=5, scoring='accuracy')\n",
    "\n",
    "# Esecuzione della Grid Search sulla versione standardizzata dei dati di addestramento\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Iperparametri ottimali:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       400\n",
      "           1       0.85      0.85      0.85       400\n",
      "\n",
      "    accuracy                           0.85       800\n",
      "   macro avg       0.85      0.85      0.85       800\n",
      "weighted avg       0.85      0.85      0.85       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "linear_svm =  svm.SVC(kernel='poly', degree=grid_search.best_params_['degree'], C=grid_search.best_params_['C'] )\n",
    "linear_svm.fit(X_train, y_train)\n",
    "y_pred = linear_svm.predict(X_test)\n",
    "classification_report(y_test, y_pred)\n",
    "# Stampa di precisione, richiamo e F1 per ciascuna classe\n",
    "report = classification_report(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      5000\n",
      "           1       0.88      0.90      0.89      5000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# eliminiamo le review che non hanno il label considerando che vogliamo fare dell'aprendimento supervisionato\n",
    "data_filtered = data.dropna(subset=['label']).copy()\n",
    "#generiamo la matrice in base alle fetures precedentemente create su tutto il corpus e ai documenti con cui vogliamo fare la classificazione\n",
    "X = tfidf_vectorizer.transform(data_filtered['clean_review'])\n",
    "# associamo il label ad un valore numerico\n",
    "le = LabelEncoder()\n",
    "data_filtered['label_encoded'] = le.fit_transform(data_filtered['label'])\n",
    "#classificazione\n",
    "y = data_filtered['label_encoded']\n",
    "#kernel choice \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "linear_svm =  svm.SVC(kernel='linear')\n",
    "linear_svm.fit(X_train, y_train)\n",
    "y_pred = linear_svm.predict(X_test)\n",
    "# Stampa di precisione, richiamo e F1 per ciascuna classe\n",
    "report = classification_report(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beliven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
